# x is a vector for regressors
# y is a vector for observed response data
lare <- function(x, y) {
# search for the LARE numerically by optim
params <- c(1, 1)
opt <- optim(
par = params,
# initial guess for parameters
fn = function(params, data) {sum (abs(y - params[1] + params[2] * x)) }
# the function to be minimized
)
# return the results
return(opt$par)
}
# 4.16
plot(reg.data.outlier$x, reg.data.outlier$y)
points(reg.data$x, reg.data$y, pch = 16)
# reg.data
par(mfrow = c(1, 2))
normal_fit <- lm(y ~ x,
data = reg.data)
fitted <- predict(normal_fit,
newdata = data.frame(x = reg.data$x))
residuals <- reg.data$y - fitted
plot(fitted, residuals)
laplace_fit[1] + laplace_fit[2] * reg.data$x
# outlier.data
out_normal_fit <- lm(y ~ x,
data = reg.data.outlier)
out_laplace_fit <- lare(reg.data.outlier$x, reg.data.outlier$y)
par(mfrow = c(1, 2))
plot(normal_fit, which = 1)
optim_par <- mle_laplace(x = data)
plot(sort(intervals, decreasing = T))
rm(list = ls())
# Set n <- a thousand
n <- 1e3
# Generate u
u <- c(0, sort(runif(n)), 1)
# Compute the intervals between u(i) and u(i-1)
intervals <- diff(u)
plot(sort(intervals, decreasing = T))
mllk_exp <- function(lambda, data) {
-sum(log(lambda) - lambda * data)
}
# Optimize the rate parameter in the interval 0, 1000
optimize(mllk_exp, interval = c(0, 10e3), data = intervals)
1 / mean(intervals) # should return the same
optimize(mllk_exp, interval = c(0, 1e3), data = intervals)
# Optimize the rate parameter in the interval 0, 10000
optimize(mllk_exp, interval = c(0, 1e4), data = intervals)
1 / mean(intervals) # should return the same
hist(intervals * (n + 1), breaks = 30, prob = T)
curve(dexp(x, rate = 1), col = "red", add = T)
# Density line of data + theoretical curve
plot(density(intervals * (n + 1)), ylim = c(0, 1))
curve(dexp(x, rate = 1), col = "red", add = T)
# 2
rm(list = ls())
# Set n <- a thousand
n <- 1e3
# Generate u
u <- c(0, sort(runif(n)), 1)
# Compute the intervals between u(i) and u(i-1)
intervals <- diff(u)
plot(sort(intervals, decreasing = T))
# Looks exponential ...
# Define the function to be optimized in -log likelihood estimation
mllk_exp <- function(lambda, data) {
-sum(log(lambda) - lambda * data)
}
# Optimize the rate parameter in the interval 0, 10000
optimize(mllk_exp, interval = c(0, 1e4), data = intervals)
1 / mean(intervals) # being the analytical expression
# of the ML estimation, should return the same
# It does!
# Model checking
# Histogram of data + theoretical curve
hist(intervals * (n + 1), breaks = 30, prob = T)
curve(dexp(x, rate = 1), col = "red", add = T)
plot(density(intervals * (n + 1)), ylim = c(0, 1))
curve(dexp(x, rate = 1), col = "red", add = T)
N <- n + 1
theoretical <- qexp( (1:N-0.5) / N)
plot(theoretical, sort(intervals))
abline(0, 1/N)
# Clean environment
rm(list = ls())
# Load data
Cerio <- read.table(
"http://www.math.ku.dk/~tfb525/teaching/statbe/Ceriodaphnia.txt",
header=TRUE)
attach(Cerio)
# fit0 <- lm(organisms ~ concentration,
#            data = Cerio)
# Fit using simple linear Poisson model
fit1 <- glm( organisms ~ concentration,
data = Cerio,
family = poisson("log"))
# Fit using polynomial degree 2 Poisson model
fit2 <- glm( organisms ~ poly(concentration, 2),
data = Cerio,
family = poisson("log"))
# Run anova using LRT test
anova(fit1, fit2, test = "LRT")
#Null hypothesis cannot be rejected -> the nested model is enough
# par(mfrow = c(1, 3))
# plot(predict(fit0))
# points(0:34, organisms, pch = 16)
#
# plot(exp(predict(fit1)))
# points(0:34, organisms, pch = 16)
#
# plot(exp(predict(fit2)))
# points(0:34, organisms, pch = 16)
#3.2
# Clean environment
#3.2
# Clean environment
rm(list = ls())
# Load data
load(url("http://www.math.ku.dk/~tfb525/teaching/statbe/flies.RData"))
attach(fly.death)
# Fit using simple linear Logistic model
fit1 <- glm( dead ~ log(conc),
data = fly.death,
family = binomial("logit"))
# Fit using polynomial degree 2 Logistic model
fit2 <- glm( dead ~ poly(log(conc), 2),
data = fly.death,
family = binomial("logit"))
# Run anova using LRT test
anova(fit1, fit2, test = "LRT")
mu <- 0
mu <- 0
b <- 2
N <- 1e3
x <- (1:N-0.5) / N
theoretical <- qlaplace(x, mu, b)
plot(theoretical, sort(rlaplace(N, mu, b)))
plaplace <- function(x, mu, b) {
ifelse(x < mu, { 0.5 * exp( (x - mu) / b) }, { 1 - 0.5 * exp( -(x - mu) / b) })
}
dlaplace <- function(x, mu, b) {
(0.5 / b) * exp( -(abs(x - mu) / b))
}
qlaplace <- function(p, mu, b) {
ifelse(p < 0.5, b * log(2 * p) + mu, -b * log(-2 * p + 2) + mu )
}
rlaplace <- function(n, mu = 0, b = 2) {
qlaplace(runif(n), mu, b)
}
mu <- 0
b <- 2
N <- 1e3
x <- (1:N-0.5) / N
theoretical <- qlaplace(x, mu, b)
plot(theoretical, sort(rlaplace(N, mu, b)))
abline(0, 1)
curve(dlaplace(x, mu = mu, b = b), from = -10, to = +10)
curve(plaplace(x, mu = mu, b = b), from = -10, to = +10)
curve(qlaplace(x, mu = mu, b = b), from = 0, to = +1)
curve(dlaplace(x, mu = mu, b = b), from = -10, to = +10)
curve(dlaplace(x, mu = mu, b = b), from = -10, to = +10,
main = "Laplace PDF", ylab = "Probability")
curve(dlaplace(x, mu = mu, b = b), from = -10, to = +10,
main = "Laplace PDF", ylab = "Probability")
curve(plaplace(x, mu = mu, b = b), from = -10, to = +10,
main = "Laplace CDF", ylab = "Probability")
curve(qlaplace(x, mu = mu, b = b), from = 0, to = +1,
main = "Laplace Quantile function", ylab = "Percentil")
curve(qlaplace(x, mu = mu, b = b), from = 0, to = +1,
main = "Laplace Quantile function", xlab = "Percentil")
curve(qlaplace(x, mu = mu, b = b), from = 0, to = +1,
main = "Laplace Quantile function", xlab = "Percentil", ylab = "y")
curve(dlaplace(x, mu = mu, b = b), from = -10, to = +10,
main = "Laplace PDF", ylab = "Probability")
curve(plaplace(x, mu = mu, b = b), from = -10, to = +10,
main = "Laplace CDF", ylab = "Probability")
curve(qlaplace(x, mu = mu, b = b), from = 0, to = +1,
main = "Laplace Quantile function", xlab = "Percentil", ylab = "y")
# 4.7
E_x <- integrate( function(x) { dlaplace(x, mu, b) * x}, lower = -Inf, upper = +Inf)$value
V_x <- integrate( function(x) { dlaplace(x, mu, b) * (x - E_x) ^ 2 }, lower = -Inf, upper = +Inf)$value
mll_laplace <- function(params, data) {
mu <- params[1]
b <- params[2]
result <- sum(length(data) * log(2 * b) + (1 / b) * sum(abs(data - mu)))
return(result)
}
mle_laplace <- function(x) {
params <- c(1, 1)
# compute the mle of mu and b
result <- optim(par = params,
mll_laplace,
data = x)
# return the results
result.par <- result$par
return(result.par)
}
optim_par <- mle_laplace(x = data)
data
data
load("~/MEGA/Master/STATS/exam/exam.RData")
optim_par <- mle_laplace(x = data)
# qqplot
N <- length(data)
theoretical <- qlaplace(((1:N - 0.5) / N), optim_par[1], optim_par[2])
plot(theoretical, sort(data))
abline(0, 1)
mu <- optim_par[1]
b <- optim_par[2]
B = 100; n = length(data)
bootstrap_data = matrix(rlaplace(B*n, mu, b), ncol = B)
# obtain the B bootstrapping estimates
bootstrap_t = apply(bootstrap_data, 2, mle_laplace)
# kernel density plot and confidence interval for mu
plot(density(bootstrap_t[1, ]), lwd=2)
plot(density(bootstrap_t[1, ]), lwd=2,
main = "Bootstrap CI for mu")
abline(v=quantile(bootstrap_t[1, ], c(0.025, 0.975)),
lty=2, lwd=2)
quantile(bootstrap_t[1, ], c(0.025, 0.975))
plot(density(bootstrap_t[2, ]), lwd=2,
main = "Bootstraip CI for b")
abline(v=quantile(bootstrap_t[2, ], c(0.025, 0.975)),
lty=2, lwd=2)
quantile(bootstrap_t[2, ], c(0.025, 0.975))
N <- 1000
a <- runif(1)
b <- runif(1)
epsilon <- rlaplace(N, 0, 1)
st_e <- a * epsilon + b
theoretical <- qlaplace((1:N - 0.5) / N, a, b)
plot(sort(epsilon), theoretical)
plot(sort(epsilon), theoretical, main = "QQplot")
theoretical <- qlaplace((1:N - 0.5) / N, a, b)
plot(sort(epsilon), theoretical, main = "QQplot")
N <- 1000
a <- runif(1)
b <- runif(1)
epsilon <- rlaplace(N, 0, 1)
st_e <- a * epsilon + b
theoretical <- qlaplace((1:N - 0.5) / N, a, b)
plot(sort(epsilon), theoretical, main = "QQplot")
abline(0, 1)
abline(0, 1/N)
abline(0, 1)
theoretical <- qlaplace((1:N - 0.5) / N, a, b)
plot(sort(epsilon), theoretical, main = "QQplot")
abline(0, 1)
abline(0, 1/2)
abline(0.5, 1/2)
abline(0, 1)
plot(sort(epsilon), theoretical, main = "QQplot")
abline(0, 1)
plot(sort(epsilon), theoretical, main = "qqplot")
abline(0, 1)
plot(reg.data.outlier$x, reg.data.outlier$y)
points(reg.data$x, reg.data$y, pch = 16)
par(mfrow = c(1, 2))
normal_fit <- lm(y ~ x,
data = reg.data)
fitted <- predict(normal_fit,
newdata = data.frame(x = reg.data$x))
residuals <- reg.data$y - fitted
plot(fitted, residuals)
fitted
residuals
?readPNG
library("png")
library("grid")
?grid.raster
i
# Analysis of fragments results from protein.py
# Load libraries and set variables
setwd("/home/antortjim/MEGA/Master/SB/exam")
library("ggplot2")
library("dtplyr")
library("data.table")
library("cowplot")
theme_set(theme_bw(base_size = 20))
flen <- 14
out_folder = "shiny/sb_exam/out/"
plot_folder = "plots/"
check_intraprotein <- function(my_row) {
if (my_row[2] == my_row[6]) {
if(my_row[3] == my_row[7] & my_row[4] == my_row[8]) {
return(NA)
} else {
return(TRUE)
}
} else {
return(FALSE)
}
}
# read data
sequence_fn <- paste(out_folder, flen, "-mers_rmsd.txt", sep = "")
random_fn <- paste(out_folder, flen, "-mers_random.txt", sep = "")
fragments_fn <- paste(out_folder, flen, "-mer_fragments.csv", sep = "")
print("Reading data")
sequence <- read.csv(file = sequence_fn, header = T)
sequence$pair <- "sequence"
random <- read.csv(file = random_fn, header = T)
random$pair <- "random"
fragments <- read.csv(file = fragments_fn, header = T)
# Pymol
# Select residues in pdb4ptp and pdb1mct
# pdb1 <- "pdb4ptp.ent"
# pdb2 <- "pdb1mct.ent"
#
#
# (my_data %>% filter(f1.seq_id == pdb1 &
#                      f2.seq_id == pdb2) %>%
#   arrange(f1.start))$f1.start
#
# (my_data %>% filter(f1.seq_id == pdb1 &
#                       f2.seq_id == pdb2) %>%
#   arrange(f2.start))$f2.start
#
#
pdb1 <- "pdb1cpc.ent"
pdb2 <- "pdb2rhe.ent"
####################################################################
# Reload data and repeat analysis without excluding duplicates as above
#my_data <- full_join(sequence, random)
# Mark intraprotein pairs (pairs in the same protein)
# Could be same chain or not (never in the same position exactly)
sequence$intraprotein <- apply(sequence, 1, check_intraprotein)
# Mark duplicates
# Duplicate = two or more entries where f1 and f2.seq_id
# are identical
sequence$id <- 1:nrow(sequence)
sequence$duplicated <- TRUE
DT <- as.data.table(sequence)
DT[DT[,.SD[1], by=.(f1.seq_id, f2.seq_id)]$id, 13] <- FALSE
sequence <- DT
# Merge sequence and random data
#idx <- sample(x = 1:nrow(random), size = nrow(sequence))
#random_subset <- random[idx, ]
random$duplicated <- F
my_data <- full_join(sequence, as.data.table(random))
# Use formatted labels in facetting
pair_names <- c(
'random' = "Random pairs",
'sequence' = "Sequence pairs")
print("Plotting histograms")
A <- ggplot(data = my_data, aes(rmsd, fill = pair)) +
geom_histogram(bins = 60) +
facet_wrap( ~ pair, labeller = as_labeller(pair_names)) +
guides(fill = FALSE) +
labs(x = "RMSD", y = "Count")
A
source('~/MEGA/Master/SB/exam/analysis/analysis.R', echo=TRUE)
A
B
out_folder = "shiny/sb_exam/out/"
plot_folder = "plots/"
check_intraprotein <- function(my_row) {
if (my_row[2] == my_row[6]) {
if(my_row[3] == my_row[7] & my_row[4] == my_row[8]) {
return(NA)
} else {
return(TRUE)
}
} else {
return(FALSE)
}
}
# read data
sequence_fn <- paste(out_folder, flen, "-mers_rmsd.txt", sep = "")
random_fn <- paste(out_folder, flen, "-mers_random.txt", sep = "")
fragments_fn <- paste(out_folder, flen, "-mer_fragments.csv", sep = "")
print("Reading data")
sequence <- read.csv(file = sequence_fn, header = T)
sequence$pair <- "sequence"
random <- read.csv(file = random_fn, header = T)
random$pair <- "random"
fragments <- read.csv(file = fragments_fn, header = T)
# Pymol
# Select residues in pdb4ptp and pdb1mct
# pdb1 <- "pdb4ptp.ent"
# pdb2 <- "pdb1mct.ent"
#
#
# (my_data %>% filter(f1.seq_id == pdb1 &
#                      f2.seq_id == pdb2) %>%
#   arrange(f1.start))$f1.start
#
# (my_data %>% filter(f1.seq_id == pdb1 &
#                       f2.seq_id == pdb2) %>%
#   arrange(f2.start))$f2.start
#
#
pdb1 <- "pdb1cpc.ent"
pdb2 <- "pdb2rhe.ent"
####################################################################
# Reload data and repeat analysis without excluding duplicates as above
#my_data <- full_join(sequence, random)
# Mark intraprotein pairs (pairs in the same protein)
# Could be same chain or not (never in the same position exactly)
sequence$intraprotein <- apply(sequence, 1, check_intraprotein)
# Mark duplicates
# Duplicate = two or more entries where f1 and f2.seq_id
# are identical
sequence$id <- 1:nrow(sequence)
sequence$duplicated <- TRUE
DT <- as.data.table(sequence)
DT[DT[,.SD[1], by=.(f1.seq_id, f2.seq_id)]$id, 13] <- FALSE
sequence <- DT
# Merge sequence and random data
#idx <- sample(x = 1:nrow(random), size = nrow(sequence))
#random_subset <- random[idx, ]
random$duplicated <- F
my_data <- full_join(sequence, as.data.table(random))
# Use formatted labels in facetting
pair_names <- c(
'random' = "Random pairs",
'sequence' = "Sequence pairs")
print("Plotting histograms")
A <- ggplot(data = my_data, aes(rmsd, fill = pair)) +
geom_histogram(bins = 60) +
facet_wrap( ~ pair, labeller = as_labeller(pair_names)) +
guides(fill = FALSE) +
labs(x = "RMSD", y = "Count")
A
B
flen <- 5
out_folder = "shiny/sb_exam/out/"
plot_folder = "plots/"
check_intraprotein <- function(my_row) {
if (my_row[2] == my_row[6]) {
if(my_row[3] == my_row[7] & my_row[4] == my_row[8]) {
return(NA)
} else {
return(TRUE)
}
} else {
return(FALSE)
}
}
# read data
sequence_fn <- paste(out_folder, flen, "-mers_rmsd.txt", sep = "")
random_fn <- paste(out_folder, flen, "-mers_random.txt", sep = "")
fragments_fn <- paste(out_folder, flen, "-mer_fragments.csv", sep = "")
print("Reading data")
sequence <- read.csv(file = sequence_fn, header = T)
sequence$pair <- "sequence"
random <- read.csv(file = random_fn, header = T)
random$pair <- "random"
fragments <- read.csv(file = fragments_fn, header = T)
# Pymol
# Select residues in pdb4ptp and pdb1mct
# pdb1 <- "pdb4ptp.ent"
# pdb2 <- "pdb1mct.ent"
#
#
# (my_data %>% filter(f1.seq_id == pdb1 &
#                      f2.seq_id == pdb2) %>%
#   arrange(f1.start))$f1.start
#
# (my_data %>% filter(f1.seq_id == pdb1 &
#                       f2.seq_id == pdb2) %>%
#   arrange(f2.start))$f2.start
#
#
pdb1 <- "pdb1cpc.ent"
pdb2 <- "pdb2rhe.ent"
####################################################################
# Reload data and repeat analysis without excluding duplicates as above
#my_data <- full_join(sequence, random)
# Mark intraprotein pairs (pairs in the same protein)
# Could be same chain or not (never in the same position exactly)
sequence$intraprotein <- apply(sequence, 1, check_intraprotein)
# Mark duplicates
# Duplicate = two or more entries where f1 and f2.seq_id
# are identical
sequence$id <- 1:nrow(sequence)
sequence$duplicated <- TRUE
DT <- as.data.table(sequence)
DT[DT[,.SD[1], by=.(f1.seq_id, f2.seq_id)]$id, 13] <- FALSE
sequence <- DT
# Merge sequence and random data
#idx <- sample(x = 1:nrow(random), size = nrow(sequence))
#random_subset <- random[idx, ]
random$duplicated <- F
my_data <- full_join(sequence, as.data.table(random))
# Use formatted labels in facetting
pair_names <- c(
'random' = "Random pairs",
'sequence' = "Sequence pairs")
print("Plotting histograms")
A <- ggplot(data = my_data, aes(rmsd, fill = pair)) +
geom_histogram(bins = 60) +
facet_wrap( ~ pair, labeller = as_labeller(pair_names)) +
guides(fill = FALSE) +
labs(x = "RMSD", y = "Count")
A
#fn <- paste(plot_folder, "duplicates_histogram.png", sep = "")
#ggsave(fn, width = 13, height = 6)
# Select nonduplicated data and same amount of fragments in random
n <- sum(!sequence$duplicated)
my_data <- arrange(my_data, -pair)
DT <- my_data[c(1:(nrow(sequence) + n)),]
B <- ggplot(data = filter(DT, duplicated == F), aes(rmsd, fill = pair)) +
geom_histogram(bins = 60) +
facet_wrap( ~ pair, labeller = as_labeller(pair_names)) +
guides(fill = FALSE) +
labs(x = "RMSD", y = "Count")
B
A
B
